# Lab 3.5: Kubernetes Under the Hood
Understand how Kubernetes actually works: controller loops, etcd, API server flow, scheduler, and kubelet.

**Time**: 50 minutes  
**Difficulty**: â­â­â­ Advanced  
**Focus**: Kubernetes internals, Control plane, Controller patterns

---

## ğŸ¯ Objective

Demystify the "magic" of Kubernetes. Learn how a simple `kubectl apply` triggers a cascade of events across the control plane, scheduler, and kubelet to create running pods.

Most Kubernetes courses teach **what** to do (deploy apps, create services). This lab teaches **how** Kubernetes does it internally.

---

## ğŸ“‹ What You'll Learn

- **Controller loop pattern** (watch â†’ compare â†’ reconcile)
- **etcd**: The brain of Kubernetes (key-value store)
- **API server request flow** (authentication â†’ authorization â†’ admission â†’ validation)
- **What happens during `kubectl apply`?** (trace YAML â†’ running pod)
- **Scheduler decision-making** (node selection algorithm)
- **Kubelet pod lifecycle** (image pull â†’ container runtime â†’ health checks)
- **5 ways to break a deployment** (systematically break each component)

---

## ğŸ§­ Prerequisites

**Required**: Completed Labs 1-3 (basic Kubernetes concepts)  
**Helpful**: Curiosity about "how does this actually work?"

**Tools needed**:
- `kubectl` (with cluster access)
- `etcdctl` (will install in this lab)
- `watch` command (Linux/macOS)

---

## ğŸ—ï¸ Architecture: The Kubernetes Control Plane

Before diving in, let's visualize the control plane components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Kubernetes Control Plane                     â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ API Server â”‚â—„â”€â”¤ etcd (state) â”‚  â”‚Scheduler â”‚  â”‚Controller â”‚ â”‚
â”‚  â”‚  (kubectl) â”‚  â”‚  key-value   â”‚  â”‚(podâ†’node)â”‚  â”‚ Manager   â”‚ â”‚
â”‚  â”‚   talks    â”‚  â”‚    store     â”‚  â”‚ binding  â”‚  â”‚ (loops)   â”‚ â”‚
â”‚  â”‚    here    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                                  â”‚
â”‚         â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚ (API calls)
          â”‚
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Worker Nodes                              â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Node 1: kubelet (watches API server for pod assignments) â”‚   â”‚
â”‚  â”‚   â”œâ”€ Container Runtime (containerd/Docker)               â”‚   â”‚
â”‚  â”‚   â”œâ”€ Pod: nginx-7d8b49557c-abc12                         â”‚   â”‚
â”‚  â”‚   â””â”€ kube-proxy (iptables rules for Services)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Node 2: kubelet                                           â”‚   â”‚
â”‚  â”‚   â”œâ”€ Container Runtime                                    â”‚   â”‚
â”‚  â”‚   â”œâ”€ Pod: nginx-7d8b49557c-xyz89                         â”‚   â”‚
â”‚  â”‚   â””â”€ kube-proxy                                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight**: Kubernetes is a **distributed system** where components watch for changes and react independently. There's no "master orchestrator" - it's all controller loops!

---

## Phase 1: The Controller Loop Pattern (20 minutes)

### ğŸ“š Theory: Watch â†’ Compare â†’ Reconcile

**Every Kubernetes controller** (Deployment, ReplicaSet, StatefulSet, etc.) follows the same pattern:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              The Universal Controller Loop               â”‚
â”‚                                                          â”‚
â”‚   1. WATCH: Monitor API server for changes              â”‚
â”‚      (e.g., "Deployment spec changed")                  â”‚
â”‚              â–¼                                           â”‚
â”‚   2. COMPARE: Desired state vs actual state             â”‚
â”‚      (e.g., "Want 3 replicas, have 2 replicas")         â”‚
â”‚              â–¼                                           â”‚
â”‚   3. RECONCILE: Take action to match desired state      â”‚
â”‚      (e.g., "Create 1 more pod")                        â”‚
â”‚              â–¼                                           â”‚
â”‚   4. REPEAT: Loop forever (every few seconds)           â”‚
â”‚              â–¼                                           â”‚
â”‚   Back to step 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example: Deployment Controller**

```
User runs: kubectl scale deployment nginx --replicas=5

â”Œâ”€ Deployment Controller Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. WATCH: "Oh! Deployment 'nginx' spec changed"       â”‚
â”‚    Desired replicas: 5                                 â”‚
â”‚                                                        â”‚
â”‚ 2. COMPARE: Check current state                       â”‚
â”‚    kubectl get rs nginx-7d8b49557c                    â”‚
â”‚    Current replicas: 2                                 â”‚
â”‚    Status: 2 < 5 (need 3 more!)                       â”‚
â”‚                                                        â”‚
â”‚ 3. RECONCILE: Update ReplicaSet                       â”‚
â”‚    kubectl patch rs nginx-7d8b49557c -p '{"spec":     â”‚
â”‚      {"replicas":5}}'                                  â”‚
â”‚    â†’ Triggers ReplicaSet controller!                   â”‚
â”‚                                                        â”‚
â”‚ 4. REPEAT: Watch for next change...                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ ReplicaSet Controller Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. WATCH: "ReplicaSet 'nginx-7d8b49557c' changed!"    â”‚
â”‚    Desired replicas: 5                                 â”‚
â”‚                                                        â”‚
â”‚ 2. COMPARE: Count running pods                        â”‚
â”‚    kubectl get pods -l app=nginx                      â”‚
â”‚    Current pods: 2                                     â”‚
â”‚    Status: 2 < 5 (need 3 more!)                       â”‚
â”‚                                                        â”‚
â”‚ 3. RECONCILE: Create 3 new pods                       â”‚
â”‚    POST /api/v1/namespaces/default/pods               â”‚
â”‚    (3 times)                                           â”‚
â”‚    â†’ Triggers Scheduler!                               â”‚
â”‚                                                        â”‚
â”‚ 4. REPEAT: Watch for next change...                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”¬ Hands-On: Watch the Deployment Controller in Action

Let's trigger the controller loop and watch it work:

```bash
# Create namespace
kubectl create namespace controller-demo

# Deploy nginx with 1 replica
kubectl create deployment nginx --image=nginx:1.25 --replicas=1 -n controller-demo

# In Terminal 1: Watch pods in real-time
watch -n 0.5 'kubectl get pods -n controller-demo -o wide'

# In Terminal 2: Watch ReplicaSet events
kubectl get rs -n controller-demo -w

# In Terminal 3: Scale up (trigger controller loop!)
kubectl scale deployment nginx --replicas=5 -n controller-demo
```

**What you'll see**:
1. **Deployment controller** updates ReplicaSet spec (desired: 5)
2. **ReplicaSet controller** creates 4 new pods (2 â†’ 5)
3. **Scheduler** assigns each pod to a node
4. **Kubelet** (on each node) pulls image and starts containers

**Watch the timeline**:
```
T+0s:  User runs `kubectl scale`
T+1s:  Deployment controller updates ReplicaSet
T+2s:  ReplicaSet controller creates 4 pods (Pending state)
T+3s:  Scheduler assigns pods to nodes (pod.spec.nodeName set)
T+5s:  Kubelet pulls nginx:1.25 image (if not cached)
T+10s: Pods Running (all 5 replicas healthy)
```

### ğŸ› Break It: What Happens If You Delete a Pod?

```bash
# Delete one pod manually
kubectl delete pod -n controller-demo $(kubectl get pods -n controller-demo -o name | head -1)

# Watch what happens in Terminal 1 (watch kubectl get pods)
# Expected: ReplicaSet controller creates a NEW pod immediately!
```

**Why?**
1. **ReplicaSet controller loop** detects: "I have 4 pods, but spec says 5!"
2. **Reconcile**: Create 1 new pod
3. **Result**: Self-healing! Kubernetes automatically recovers.

**This is the magic of controllers**: Declarative ("I want 5") vs Imperative ("Start pod A, start pod B...")

---

## Phase 2: etcd Deep Dive (15 minutes)

### ğŸ“š Theory: etcd = The Brain of Kubernetes

**etcd is a distributed key-value store** that holds ALL Kubernetes state:
- Pod definitions
- Service configurations
- Secrets (base64-encoded)
- Node status
- ConfigMaps
- Everything!

**Critical insight**: When you `kubectl apply`, you're writing to etcd. When controllers reconcile, they read from etcd.

```
kubectl apply -f deployment.yaml
        â†“
API Server validates YAML
        â†“
Write to etcd: /registry/deployments/default/nginx
        â†“
Deployment controller watches etcd for changes
        â†“
Controller loop triggered!
```

### ğŸ”¬ Hands-On: Explore etcd Directly

**Warning**: Directly accessing etcd is **dangerous** in production! This is for learning only.

#### Step 1: Access etcd (Local Cluster Only!)

```bash
# For kind/minikube/k3d clusters, etcd runs in a pod
kubectl get pods -n kube-system | grep etcd

# Get etcd pod name
ETCD_POD=$(kubectl get pods -n kube-system -l component=etcd -o name)

# Install etcdctl inside the pod (Alpine Linux)
kubectl exec -n kube-system $ETCD_POD -- sh -c '
  apk add --no-cache etcd-ctl 2>/dev/null || true
'
```

#### Step 2: Read Kubernetes State from etcd

```bash
# List all keys in etcd (warning: LOTS of output!)
kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  get / --prefix --keys-only | head -20

# Example output:
# /registry/deployments/default/nginx
# /registry/pods/default/nginx-7d8b49557c-abc12
# /registry/services/default/kubernetes
# /registry/secrets/default/my-secret
```

#### Step 3: Watch etcd Changes in Real-Time

```bash
# In Terminal 1: Watch etcd for pod changes
kubectl exec -n kube-system $ETCD_POD -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  watch --prefix /registry/pods/controller-demo/

# In Terminal 2: Create a pod
kubectl run test-pod --image=nginx -n controller-demo

# Terminal 1 will show: PUT /registry/pods/controller-demo/test-pod
# (etcd records the pod creation!)
```

### ğŸ“ Key Takeaway: etcd is the Source of Truth

- **API server** is stateless (can restart without losing data)
- **etcd** is stateful (lose etcd = lose entire cluster state)
- **Backup etcd** = backup entire Kubernetes cluster
- **etcd failure** = total cluster failure (no reads/writes possible)

**Production tip**: Always run etcd with 3 or 5 replicas (HA with Raft consensus).

---

## Phase 3: kubectl apply Lifecycle (10 minutes)

### ğŸ“š Theory: From YAML to Running Pod

What happens when you run `kubectl apply -f deployment.yaml`?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: kubectl parses YAML and sends JSON to API server      â”‚
â”‚   POST /apis/apps/v1/namespaces/default/deployments           â”‚
â”‚   Body: {"apiVersion":"apps/v1","kind":"Deployment",...}       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: API Server validates request                          â”‚
â”‚   âœ… Authentication: Is this user valid? (check kubeconfig)   â”‚
â”‚   âœ… Authorization: Can this user create deployments? (RBAC)  â”‚
â”‚   âœ… Admission: Run webhooks (e.g., Pod Security Standards)   â”‚
â”‚   âœ… Validation: Is YAML schema valid? (missing fields?)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: API Server writes to etcd                             â”‚
â”‚   etcdctl put /registry/deployments/default/nginx <JSON>      â”‚
â”‚   Returns: HTTP 201 Created                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 4: Deployment Controller detects change (watch event)    â”‚
â”‚   Controller loop: "New deployment! Create ReplicaSet!"       â”‚
â”‚   POST /apis/apps/v1/namespaces/default/replicasets           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 5: ReplicaSet Controller detects change                  â”‚
â”‚   Controller loop: "Need 3 pods! Create them!"                â”‚
â”‚   POST /api/v1/namespaces/default/pods (3 times)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 6: Scheduler detects unscheduled pods                    â”‚
â”‚   For each pod: Find best node (CPU, memory, affinity rules)  â”‚
â”‚   PATCH /api/v1/namespaces/default/pods/nginx-xxx             â”‚
â”‚   Set: spec.nodeName = "worker-node-2"                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 7: Kubelet (on worker-node-2) detects pod assignment     â”‚
â”‚   Kubelet watches: "Pod nginx-xxx assigned to me!"            â”‚
â”‚   1. Pull image: docker pull nginx:1.25                       â”‚
â”‚   2. Create container: docker run nginx:1.25                  â”‚
â”‚   3. Start health checks: HTTP GET /healthz                   â”‚
â”‚   4. Report status to API server: Pod Running                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
                    Pod Running! âœ…
```

### ğŸ”¬ Hands-On: Trace the Lifecycle

```bash
# In Terminal 1: Watch API server logs (if accessible)
# kubectl logs -n kube-system kube-apiserver-xxx --follow

# In Terminal 2: Watch events in real-time
kubectl get events -n controller-demo --watch

# In Terminal 3: Deploy nginx
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: lifecycle-demo
  namespace: controller-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: lifecycle-demo
  template:
    metadata:
      labels:
        app: lifecycle-demo
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
        ports:
        - containerPort: 80
EOF
```

**Watch Terminal 2** (events) to see the cascade:
```
0s    Normal  ScalingReplicaSet  deployment/lifecycle-demo  Scaled up replica set lifecycle-demo-7d8b49557c to 2
0s    Normal  SuccessfulCreate   replicaset/lifecycle-demo  Created pod: lifecycle-demo-7d8b49557c-abc12
0s    Normal  SuccessfulCreate   replicaset/lifecycle-demo  Created pod: lifecycle-demo-7d8b49557c-xyz89
1s    Normal  Scheduled          pod/lifecycle-demo-abc12   Successfully assigned to node worker-1
1s    Normal  Scheduled          pod/lifecycle-demo-xyz89   Successfully assigned to node worker-2
3s    Normal  Pulling            pod/lifecycle-demo-abc12   Pulling image "nginx:1.25"
5s    Normal  Pulled             pod/lifecycle-demo-abc12   Successfully pulled image
6s    Normal  Created            pod/lifecycle-demo-abc12   Created container nginx
6s    Normal  Started            pod/lifecycle-demo-abc12   Started container nginx
```

**Each event = a different controller taking action!**

---

## Phase 4: The Scheduler's Decision-Making (5 minutes)

### ğŸ“š Theory: How Does Scheduler Choose a Node?

When a pod is created without `spec.nodeName`, the **Scheduler** must pick a node:

```
â”Œâ”€ Scheduler Algorithm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                            â”‚
â”‚ 1. FILTER: Eliminate unsuitable nodes                     â”‚
â”‚    âŒ Node has insufficient CPU/memory                    â”‚
â”‚    âŒ Node has no matching labels (nodeSelector)          â”‚
â”‚    âŒ Node is cordoned (kubectl cordon)                   â”‚
â”‚    âŒ Pod has anti-affinity with existing pods on node    â”‚
â”‚                                                            â”‚
â”‚ 2. SCORE: Rank remaining nodes (0-100 points)             â”‚
â”‚    ğŸ“Š Spread pods evenly (avoid hotspots)                 â”‚
â”‚    ğŸ“Š Prefer nodes with image already cached              â”‚
â”‚    ğŸ“Š Respect pod affinity/anti-affinity preferences      â”‚
â”‚    ğŸ“Š Balance resource usage (CPU/memory)                 â”‚
â”‚                                                            â”‚
â”‚ 3. BIND: Choose highest-scoring node                      â”‚
â”‚    PATCH /api/v1/namespaces/default/pods/nginx-xxx        â”‚
â”‚    Set: spec.nodeName = "worker-node-3"                   â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”¬ Hands-On: Watch Scheduler Work

```bash
# Create a pod with high CPU request (scheduler must find node with capacity)
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: high-cpu-pod
  namespace: controller-demo
spec:
  containers:
  - name: stress
    image: polinux/stress
    command: ["stress"]
    args: ["--cpu", "2"]
    resources:
      requests:
        cpu: "2000m"  # 2 full CPU cores
        memory: "1Gi"
EOF

# Watch scheduler events
kubectl get events -n controller-demo --field-selector involvedObject.name=high-cpu-pod

# Expected event:
# Normal  Scheduled  pod/high-cpu-pod  Successfully assigned to node worker-3
#   Reason: worker-3 has 4 CPUs available, workers 1-2 only have 1 CPU free
```

---

## Phase 5: 5 Ways to Break a Deployment (Debugging Practice!)

Now that you understand the internals, let's **intentionally break things** to learn how failures manifest:

### ğŸ› Break #1: Overload etcd (Simulate etcd Failure)

```bash
# This simulates what happens when etcd is unreachable

# Stop etcd (DON'T DO THIS IN PRODUCTION!)
kubectl delete pod -n kube-system $(kubectl get pods -n kube-system -l component=etcd -o name)

# Try to create a pod
kubectl run test --image=nginx -n controller-demo

# Expected result: Hangs forever!
# Why? API server can't write to etcd, so request never completes.

# Symptoms in real clusters:
# - kubectl commands timeout
# - Controllers stop reconciling (can't read state)
# - Cluster appears "frozen"

# Fix: etcd pod restarts automatically (StatefulSet with restart policy)
# Wait 30 seconds, then retry kubectl command
```

### ğŸ› Break #2: Crash the Scheduler

```bash
# Delete scheduler pod
kubectl delete pod -n kube-system $(kubectl get pods -n kube-system -l component=kube-scheduler -o name)

# Create a pod
kubectl run unscheduled-pod --image=nginx -n controller-demo

# Check pod status
kubectl get pods -n controller-demo unscheduled-pod

# Expected: STATUS = Pending forever!
# Why? No scheduler to assign nodeName.

# Check events
kubectl describe pod unscheduled-pod -n controller-demo
# Expected event: "0/3 nodes are available: waiting for scheduler"

# Fix: Scheduler pod restarts automatically
# Once restarted, pod will be scheduled within seconds
```

### ğŸ› Break #3: Kill Controller Manager (Controllers Stop Reconciling)

```bash
# Delete controller-manager pod
kubectl delete pod -n kube-system $(kubectl get pods -n kube-system -l component=kube-controller-manager -o name)

# Scale a deployment
kubectl scale deployment nginx --replicas=10 -n controller-demo

# Watch pods
kubectl get pods -n controller-demo -w

# Expected: Pods DON'T scale up! (stuck at old replica count)
# Why? Deployment controller isn't running, so no reconciliation.

# Fix: Controller-manager pod restarts automatically
# Once restarted, scaling resumes immediately
```

### ğŸ› Break #4: Disable Kubelet on a Node (Pods Can't Start)

```bash
# This simulates node failure

# Cordon a node (prevent new pods from scheduling there)
NODE=$(kubectl get nodes -o name | head -1)
kubectl cordon $NODE

# Try to schedule a pod
kubectl run node-test --image=nginx -n controller-demo

# If all nodes are cordoned, pod stays Pending
# Events: "0/3 nodes are available: 3 node(s) were unschedulable"

# Uncordon the node
kubectl uncordon $NODE

# Pod should schedule within seconds
```

### ğŸ› Break #5: Invalid YAML (API Server Rejects It)

```bash
# Try to create a deployment with invalid YAML
kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: broken-deployment
  namespace: controller-demo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: WRONG-LABEL  # âŒ Selector doesn't match template labels!
    spec:
      containers:
      - name: nginx
        image: nginx:1.25
EOF

# Expected error:
# Error: deployment.apps "broken-deployment" is invalid:
#   spec.template.metadata.labels: Invalid value: map[string]string{"app":"WRONG-LABEL"}:
#   `selector` does not match template `labels`

# This is caught by API server validation (before etcd write)
```

---

## ğŸ’¡ Key Takeaways

### 1. **Controller Loop Pattern is Universal**
Every Kubernetes resource (Deployment, StatefulSet, Service) has a controller that:
- **Watches** for changes
- **Compares** desired vs actual state
- **Reconciles** by taking action
- **Repeats** forever

### 2. **etcd is the Source of Truth**
- API server is stateless (requests go through it)
- etcd stores ALL cluster state
- Controllers read from etcd to make decisions
- **Backup etcd = backup entire cluster**

### 3. **kubectl apply is a Multi-Step Process**
```
YAML â†’ kubectl â†’ API Server â†’ etcd â†’ Controllers â†’ Scheduler â†’ Kubelet â†’ Running Pod
```
Each step can fail independently!

### 4. **Failures Manifest Differently**
- **etcd down**: All kubectl commands hang
- **Scheduler down**: Pods stuck in Pending (no nodeName assigned)
- **Controller-manager down**: Deployments don't scale, self-healing stops
- **Kubelet down**: Pods on that node become NotReady
- **API server down**: Nothing works (no communication possible)

### 5. **Self-Healing is Automatic**
- Delete a pod â†’ ReplicaSet controller creates new one
- Node fails â†’ Pods rescheduled to healthy nodes
- Control plane pod crashes â†’ Kubernetes restarts it (static pods)

---

## ğŸ”¨ Challenge: Build Your Own Controller (Optional)

Want to truly understand controllers? **Write your own!**

Here's a simple controller that watches for pods with label `auto-delete: true` and deletes them after 60 seconds:

```python
# simple-controller.py
import time
from kubernetes import client, config, watch

config.load_kube_config()
v1 = client.CoreV1Api()

print("ğŸš€ Starting simple controller...")
print("Watching for pods with label 'auto-delete: true'")

w = watch.Watch()
created_pods = {}  # Track pod creation time

# Controller loop!
for event in w.stream(v1.list_namespaced_pod, namespace="controller-demo"):
    pod = event['object']
    event_type = event['type']
    
    # Check if pod has auto-delete label
    if pod.metadata.labels and pod.metadata.labels.get('auto-delete') == 'true':
        pod_name = pod.metadata.name
        
        if event_type == 'ADDED':
            # WATCH: New pod detected!
            created_pods[pod_name] = time.time()
            print(f"â±ï¸  Pod {pod_name} created, will delete in 60s")
        
        # COMPARE: Has 60 seconds passed?
        if pod_name in created_pods:
            elapsed = time.time() - created_pods[pod_name]
            if elapsed > 60:
                # RECONCILE: Delete the pod!
                print(f"ğŸ’£ Deleting pod {pod_name} (lived for {elapsed:.0f}s)")
                v1.delete_namespaced_pod(pod_name, "controller-demo")
                del created_pods[pod_name]
```

**Test it**:
```bash
# Run controller
python3 simple-controller.py

# In another terminal, create a pod with auto-delete label
kubectl run auto-delete-test --image=nginx --labels=auto-delete=true -n controller-demo

# Watch: Pod will be deleted after 60 seconds!
```

**This is how Kubernetes works!** Every controller is just a loop watching for changes and taking action.

---

## ğŸ§¹ Cleanup

```bash
# Delete namespace
kubectl delete namespace controller-demo

# This triggers a cascade:
# 1. Namespace controller deletes all resources in namespace
# 2. Deployment controllers stop managing deployments
# 3. ReplicaSet controllers delete pods
# 4. Kubelet stops containers
# 5. etcd removes all keys under /registry/.../controller-demo/
```

---


---

## ğŸ–ï¸ Expert Mode: Control Plane Performance Tuning

> ğŸ’¡ **Optional Challenge** â€” Ready to tune production clusters? **This is NOT required** to progress, but completing it unlocks the **âš™ï¸ Control Plane Architect** badge!

**â±ï¸ Time**: +25 minutes  
**ğŸ¯ Difficulty**: â­â­â­â­â­ (Expert)  
**ğŸ“‹ Prerequisites**: Complete Lab 3.5 etcd Deep Dive section above

### The Scenario

Your API server is timing out. `kubectl get pods` takes 5+ seconds. Prometheus shows high etcd disk latency:

```bash
# etcd_disk_wal_fsync_duration_seconds: 45ms (should be <10ms)
# etcd_mvcc_db_total_size_in_bytes: 8.5GB (massive!)
```

**Your cluster is slow because etcd is overwhelmed.**

### Challenge: Tune etcd for Production Performance

**Your Mission**:
1. Check etcd metrics (disk latency, DB size)
2. Compact old revisions
3. Defragment to reclaim space
4. Configure auto-compaction
5. Optimize disk I/O

**Hints**:
```bash
# Port-forward to etcd
kubectl port-forward -n kube-system etcd-<node> 2379:2379

# Check metrics
curl http://localhost:2379/metrics | grep etcd_disk

# Check DB size
ETCDCTL_API=3 etcdctl endpoint status --write-out=table

# Compact (keeps last 1000 revisions)
ETCDCTL_API=3 etcdctl compact <revision-1000>

# Defragment
ETCDCTL_API=3 etcdctl defrag --cluster
```

### Expected Outcome

- âœ… etcd disk latency < 10ms
- âœ… Database size reduced by 50%+
- âœ… API server response time < 500ms
- âœ… Auto-compaction configured

### Deep Dive: What You're Learning

**Production Skills**:
- etcd performance diagnostics
- Compaction strategies
- Disk I/O optimization
- Control plane tuning

**Interview Topics**:
- "etcd is slow, how do you diagnose and fix it?"
- "What's the difference between compaction and defragmentation?"
- "How do you monitor control plane health?"

**Real-World Impact**: **Reddit's 2023 outage** was caused by etcd running out of space due to no auto-compaction. Database grew to 12GB, API server became unresponsive. Engineers who knew this saved the platform.

### Complete Guide

For detailed etcd tuning procedures, see:  
**[Senior K8s Debugging Guide: etcd Performance](../../docs/reference/senior-k8s-debugging.md#21-etcd-performance-bottlenecks)**

### Badge Unlocked! ğŸ‰

Complete this challenge and you've earned:  
**âš™ï¸ Control Plane Architect** â€” You can tune production Kubernetes clusters!

**Track your progress**: [Lab Progress Tracker](../../docs/learning/LAB-PROGRESS.md#expert-badges)

## ğŸ“ What's Next?

Now that you understand Kubernetes internals, you can:
- **Debug faster**: Know which component is failing (scheduler vs kubelet vs controller)
- **Optimize performance**: Tune controller sync intervals, etcd performance
- **Build custom controllers**: Extend Kubernetes with your own logic (Operators!)
- **Appreciate the design**: Declarative, self-healing, resilient by default

**Recommended Next Labs**:
- **Lab 4**: Kubernetes Fundamentals (now you know the "why" behind the commands!)
- **Lab 9**: Helm Package Management (Helm generates YAML that goes through this same flow)
- **Lab 10**: GitOps with ArgoCD (ArgoCD is a controller watching Git repos!)

---

## ğŸ“š Further Reading

- **Kubernetes Design Patterns**: https://kubernetes.io/docs/concepts/architecture/
- **Controller Runtime**: https://github.com/kubernetes-sigs/controller-runtime
- **Writing Controllers**: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/
- **etcd Documentation**: https://etcd.io/docs/

**You now understand Kubernetes at a level most engineers never reach. Use this knowledge wisely!** ğŸš€
