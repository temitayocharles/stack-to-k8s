# Lab 9: Chaos Engineering
Practice resilience by injecting controlled failures and validating system recovery.

**Time**: 90 minutes  
**Difficulty**: ‚≠ê‚≠ê‚≠ê‚≠ê Expert  
**Focus**: Resilience testing, Failure scenarios, Recovery strategies

---

## üéØ Objective
Learn chaos engineering by intentionally breaking things and watching how Kubernetes recovers. Test application resilience under failure conditions.

## ÔøΩ Assumed Knowledge
**Required**: Deployments & Self-healing (Labs 1-4), Resource limits (Lab 4), Scaling (Lab 7)  
**Helpful**: Observability and monitoring concepts (Lab 8)

## ÔøΩüìã What You'll Learn
- Chaos Mesh basics
- Pod failure scenarios
- Network delays & partitions
- Resource stress testing
- DNS failures
- Recovery validation
- Resilience patterns

---

## ‚úÖ Prerequisites Check

```bash
./scripts/check-lab-prereqs.sh 8
```

Verifies `kubectl`, `helm`, and the social media manifests are available.

## ‚úÖ Success criteria

- Chaos Mesh control plane is Running and experiments can be created
- Target app recovers after injected failures (pods return to Running)
- No lingering experiments after cleanup

## üß≠ Architecture Snapshot

```mermaid
graph TD
  ChaosMesh[Chaos Mesh] -->|injects| SocialBackend
  Users-->SocialFrontend
  SocialFrontend-->SocialBackend
  SocialBackend-->Database[(PostgreSQL)]
  SocialBackend-->Cache[(Redis)]
```

## üì¶ Manifest Starter Kit

- Overlay status: `labs/manifests/lab-09/` (in progress)
- Manual approach: deploy the social media app, then install Chaos Mesh via the Helm chart referenced in this guide.

---

## üöÄ Steps

> **Lab Navigation**: This lab is organized in 3 phases for easier consumption:
> - **[Phase 1](#-phase-1-setup--pod-failure-scenarios)**: Setup + Pod failure scenarios (35 min)
> - **[Phase 2](#-phase-2-network-chaos)**: Network delays & partitions (25 min)
> - **[Phase 3](#-phase-3-resource-stress--advanced-scenarios)**: CPU/Memory stress + Combined chaos (30 min)

---

## üîß Phase 1: Setup & Pod Failure Scenarios

### 1. Install Chaos Mesh (10 min)

```bash
# Create namespace
kubectl create namespace chaos-testing

# Add Chaos Mesh repo
helm repo add chaos-mesh https://charts.chaos-mesh.org
helm repo update

# Install Chaos Mesh
helm install chaos-mesh chaos-mesh/chaos-mesh \
  --namespace=chaos-mesh \
  --create-namespace \
  --set dashboard.create=true

# Wait for Chaos Mesh
kubectl wait --for=condition=ready pod --all -n chaos-mesh --timeout=180s

# Port forward dashboard
kubectl port-forward -n chaos-mesh svc/chaos-dashboard 2333:2333 &
sleep 2  # Wait for port-forward to establish

echo "Chaos Dashboard: http://localhost:2333"
```

### 2. Deploy Test Application (5 min)

```bash
# Deploy social media app for testing
kubectl apply -f social-media-platform/k8s/ -n chaos-testing

# Scale up for better testing
kubectl scale deployment social-backend --replicas=3 -n chaos-testing
kubectl scale deployment social-frontend --replicas=2 -n chaos-testing

# Wait for all pods
kubectl wait --for=condition=ready pod --all -n chaos-testing --timeout=180s

# Verify
kubectl get pods -n chaos-testing
```

### 3. Scenario 1: Pod Kill (10 min)

```bash
# Create PodChaos - randomly kill pods
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-kill
  namespace: chaos-testing
spec:
  action: pod-kill
  mode: one
  selector:
    namespaces:
      - chaos-testing
    labelSelectors:
      app: social-backend
  scheduler:
    cron: '@every 30s'
EOF

# Watch pods being killed and recreated
watch -n 2 kubectl get pods -n chaos-testing

# Check events
kubectl get events -n chaos-testing --sort-by='.lastTimestamp'

# Verify app still accessible during chaos
kubectl port-forward -n chaos-testing svc/social-backend 8000:8000 &
sleep 2  # Wait for port-forward to establish
while true; do curl -s http://localhost:8000/api/health; sleep 5; done

# Stop chaos after 3 minutes
kubectl delete podchaos pod-kill -n chaos-testing
```

### 4. Scenario 2: Pod Failure (10 min)

```bash
# Create PodChaos - make pod fail (not killed, stays in error state)
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: pod-failure
  namespace: chaos-testing
spec:
  action: pod-failure
  mode: fixed
  value: '1'
  duration: '2m'
  selector:
    namespaces:
      - chaos-testing
    labelSelectors:
      app: social-backend
EOF

# Watch pod status
kubectl get pods -n chaos-testing -w

# Check which pod is affected
kubectl describe podchaos pod-failure -n chaos-testing

# Test app resilience (should still work with 2/3 pods)
for i in {1..10}; do
  curl -s http://localhost:8000/api/health
  sleep 1
done

# Cleanup
kubectl delete podchaos social-pod-failure-chaos -n social-media
```

**Phase 1 Complete!** ‚úÖ You've learned Chaos Mesh installation, pod kill, and pod failure scenarios.  
**Next**: [Phase 2: Network Chaos](#-phase-2-network-chaos) to test network delays and partitions.

---

## üåê Phase 2: Network Chaos

### 5. Scenario 3: Network Delay (15 min)
```

### 5. Scenario 3: Network Delay (15 min)

```bash
# Add 500ms latency to backend
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-delay
  namespace: chaos-testing
spec:
  action: delay
  mode: all
  selector:
    namespaces:
      - chaos-testing
    labelSelectors:
      app: social-backend
  delay:
    latency: '500ms'
    correlation: '100'
    jitter: '0ms'
  duration: '3m'
  direction: to
  target:
    mode: all
    selector:
      namespaces:
        - chaos-testing
EOF

# Test latency impact
echo "Before chaos (should be fast):"
time curl -s http://localhost:8000/api/health

echo "During chaos (should be slow ~500ms):"
time curl -s http://localhost:8000/api/health

# Monitor response times
for i in {1..20}; do
  time curl -s http://localhost:8000/api/health
  sleep 2
done

# Cleanup
kubectl delete networkchaos network-delay -n chaos-testing
```

### 6. Scenario 4: Network Partition (10 min)

```bash
# Partition backend from database
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: network-partition
  namespace: chaos-testing
spec:
  action: partition
  mode: all
  selector:
    namespaces:
      - chaos-testing
    labelSelectors:
      app: social-backend
  direction: to
  target:
    mode: all
    selector:
      namespaces:
        - chaos-testing
      labelSelectors:
        app: mongodb
  duration: '2m'
EOF

# Watch pod logs (should see connection errors)
kubectl logs -n chaos-testing -l app=social-backend --tail=20 -f

# Test API (should fail for database operations)
curl -s http://localhost:8000/api/posts

# Cleanup
kubectl delete networkchaos network-partition -n chaos-testing
```

**Phase 2 Complete!** ‚úÖ You've tested network delays and network partitions.  
**Next**: [Phase 3: Resource Stress](#-phase-3-resource-stress--advanced-scenarios) for CPU/memory pressure and combined chaos.

---

## üíæ Phase 3: Resource Stress & Advanced Scenarios

### 7. Scenario 5: CPU Stress (10 min)

```bash
# Stress CPU on backend pods
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos
metadata:
  name: cpu-stress
  namespace: chaos-testing
spec:
  mode: one
  selector:
    namespaces:
      - chaos-testing
    labelSelectors:
      app: social-backend
  stressors:
    cpu:
      workers: 2
      load: 80
  duration: '3m'
EOF

# Monitor CPU usage
watch -n 2 kubectl top pods -n chaos-testing

# Check if HPA scales up (if configured)
kubectl get hpa -n chaos-testing -w

# Cleanup
kubectl delete stresschaos cpu-stress -n chaos-testing
```

### 8. Scenario 6: Memory Stress (10 min)

```bash
# Stress memory (risk of OOMKill)
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: StressChaos
metadata:
  name: memory-stress
  namespace: chaos-testing
spec:
  mode: one
  selector:
    namespaces:
      - chaos-testing
    labelSelectors:
      app: social-backend
  stressors:
    memory:
      workers: 1
      size: '256MB'
  duration: '2m'
EOF

# Watch for OOMKilled
kubectl get pods -n chaos-testing -w

# Check events
kubectl get events -n chaos-testing | grep OOM

# Monitor memory
watch -n 2 kubectl top pods -n chaos-testing

# Cleanup
kubectl delete stresschaos memory-stress -n chaos-testing
```

### 9. Scenario 7: DNS Failure (10 min)

```bash
# Make DNS fail for specific domains
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: DNSChaos
metadata:
  name: dns-failure
  namespace: chaos-testing
spec:
  action: error
  mode: all
  selector:
    namespaces:
      - chaos-testing
    labelSelectors:
      app: social-backend
  patterns:
    - mongodb
    - social-database
  duration: '2m'
EOF

# Watch logs (should see DNS resolution errors)
kubectl logs -n chaos-testing -l app=social-backend --tail=50

# Test API
curl -s http://localhost:8000/api/health

# Cleanup
kubectl delete dnschaos dns-failure -n chaos-testing
```

### 10. Workflow: Combined Chaos (10 min)

```bash
# Run multiple chaos experiments sequentially
cat <<EOF | kubectl apply -f -
apiVersion: chaos-mesh.org/v1alpha1
kind: Workflow
metadata:
  name: combined-chaos
  namespace: chaos-testing
spec:
  entry: the-entry
  templates:
    - name: the-entry
      templateType: Serial
      deadline: 10m
      children:
        - pod-kill-workflow
        - network-delay-workflow
        - cpu-stress-workflow
    - name: pod-kill-workflow
      templateType: PodChaos
      deadline: 2m
      podChaos:
        action: pod-kill
        mode: one
        selector:
          namespaces:
            - chaos-testing
          labelSelectors:
            app: social-backend
    - name: network-delay-workflow
      templateType: NetworkChaos
      deadline: 3m
      networkChaos:
        action: delay
        mode: all
        selector:
          namespaces:
            - chaos-testing
          labelSelectors:
            app: social-backend
        delay:
          latency: '300ms'
    - name: cpu-stress-workflow
      templateType: StressChaos
      deadline: 3m
      stressChaos:
        mode: one
        selector:
          namespaces:
            - chaos-testing
          labelSelectors:
            app: social-backend
        stressors:
          cpu:
            workers: 1
            load: 50
EOF

# Watch workflow progress
kubectl get workflow combined-chaos -n chaos-testing -w

# Check workflow status
kubectl describe workflow combined-chaos -n chaos-testing

# Cleanup
kubectl delete workflow combined-chaos -n chaos-testing
```

**Phase 3 Complete!** ‚úÖ You've mastered CPU stress, memory stress, DNS failures, and combined chaos workflows.  
**All phases complete!** Proceed to [Validation](#-validation) to verify your chaos engineering skills.

------

## ‚úÖ Validation

```bash
# 1. Chaos Mesh installed
kubectl get pods -n chaos-mesh
# Expected: chaos-controller-manager, chaos-daemon, chaos-dashboard

# 2. Dashboard accessible
curl -s http://localhost:2333 | grep Chaos

# 3. All chaos experiments ran
kubectl get podchaos,networkchaos,stresschaos,dnschaos -n chaos-testing

# 4. App recovered after chaos
kubectl get pods -n chaos-testing
# Expected: All pods Running

# 5. No lingering chaos experiments
kubectl get chaos-mesh.org -A
# Expected: No active experiments

# 6. Check app health
curl http://localhost:8000/api/health
# Expected: 200 OK
```

**All checks pass?** ‚úÖ Lab complete!

---
## üß† Quick Check

<details>
  <summary>How do you watch chaos experiments in real time?</summary>
  ```bash
  kubectl get podchaos -n chaos-testing -w
  ```
  </details>

<details>
  <summary>Where are Chaos Mesh logs stored?</summary>
  In the controller manager:

  ```bash
  kubectl logs -n chaos-mesh deploy/chaos-controller-manager
  ```
  </details>

## üèÜ Challenge Mode

- Add latency to only 50% of requests by using the `pod-failure` selector with `mode: fixed-percent`.
- Chain experiments (pod kill + network delay) with schedules to mimic cascading failures.
- Export Chaos Mesh metrics to Prometheus for visualization.

## üîß Troubleshooting Flow

1. **Chaos Mesh CRDs missing?** ‚Üí Re-run the Helm installation with `--set installCRDs=true`.
2. **Experiments stuck in Injecting state?** ‚Üí Check controller logs for RBAC issues.
3. **Target pods unaffected?** ‚Üí Verify label selectors in the Chaos object match pod labels.
4. **Cluster instability?** ‚Üí Disable experiments quickly with `kubectl delete <chaos-kind>`.

---

## üßπ Cleanup

```bash
kubectl delete namespace chaos-testing
kubectl delete namespace chaos-mesh

# Kill port forwards
pkill -f "port-forward"
```

---

## üéì Key Concepts Learned

1. **Chaos Engineering**: Proactively test failures
2. **Pod Chaos**: Kill, fail, container kill
3. **Network Chaos**: Delay, loss, partition, duplicate
4. **Stress Chaos**: CPU, memory, IO stress
5. **DNS Chaos**: Error, random DNS responses
6. **Workflows**: Orchestrate multiple chaos scenarios
7. **Recovery**: Kubernetes self-healing in action

---

## üìö Chaos Engineering Best Practices

### Start Small
1. Test in dev/staging first
2. Start with one pod
3. Short durations (1-2 min)
4. Monitor closely

### Gradually Increase
1. Increase blast radius (mode: one ‚Üí fixed ‚Üí percentage ‚Üí all)
2. Longer durations
3. Multiple simultaneous chaos
4. Production testing (careful!)

### Always Have
- Monitoring in place
- Alerting configured
- Rollback plan
- Incident response process

---

## üîß Chaos Mesh Actions

### PodChaos
- `pod-kill`: Delete pod
- `pod-failure`: Make pod fail
- `container-kill`: Kill container

### NetworkChaos
- `delay`: Add latency
- `loss`: Drop packets
- `duplicate`: Duplicate packets
- `corrupt`: Corrupt packets
- `partition`: Network partition
- `bandwidth`: Limit bandwidth

### StressChaos
- `cpu`: CPU stress
- `memory`: Memory stress

### IOChaos
- `latency`: IO delays
- `fault`: IO errors
- `attrOverride`: Modify file attributes

### DNSChaos
- `error`: DNS resolution fails
- `random`: Return random IP

---

## üìä Monitoring During Chaos

### Key Metrics
```bash
# Pod restarts
kubectl get pods -n chaos-testing -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.containerStatuses[0].restartCount}{"\n"}{end}'

# Response times
for i in {1..100}; do
  time curl -s http://localhost:8000/api/health 2>&1 | grep real
done

# Error rates
# Monitor 4xx/5xx responses

# Resource usage
kubectl top pods -n chaos-testing
```

---

## üîç Debugging Tips

**Chaos not applying?**
```bash
# Check chaos experiment status
kubectl describe <chaos-type> <name> -n <namespace>

# Check chaos-daemon logs
kubectl logs -n chaos-mesh -l app.kubernetes.io/component=chaos-daemon

# Verify selector matches pods
kubectl get pods -n <namespace> --show-labels
```

**App not recovering?**
```bash
# Check if chaos is still active
kubectl get chaos-mesh.org -A

# Force delete chaos
kubectl delete <chaos-type> <name> -n <namespace> --force

# Restart pods
kubectl rollout restart deployment <name> -n <namespace>
```

**Dashboard not accessible?**
```bash
# Check dashboard pod
kubectl get pods -n chaos-mesh -l app.kubernetes.io/component=chaos-dashboard

# Check service
kubectl get svc -n chaos-mesh

# Try different port forward
kubectl port-forward -n chaos-mesh svc/chaos-dashboard 8080:2333
```

---

## üéØ Real-World Chaos Scenarios

### Scenario: Database Outage
```yaml
# Partition all apps from database for 5 min
action: partition
target: database
duration: 5m
```
**Expected**: Apps queue requests, retry, degrade gracefully

### Scenario: Zone Failure
```yaml
# Kill all pods in one availability zone
selector:
  labelSelectors:
    topology.kubernetes.io/zone: us-east-1a
action: pod-kill
mode: all
```
**Expected**: Traffic shifts to other zones

### Scenario: Cascading Failure
```yaml
# Stress one service, causing downstream failures
stressors:
  cpu:
    load: 100
duration: 10m
```
**Expected**: Circuit breakers prevent cascade

---

## üèÜ Congratulations!

You've completed Lab 9! You now have hands-on experience with:

‚úÖ **Lab 1**: Pods, Deployments, Services  
‚úÖ **Lab 2**: Multi-tier apps, ConfigMaps  
‚úÖ **Lab 3**: StatefulSets, Persistent storage  
‚úÖ **Lab 4**: Labels, Selectors, Troubleshooting  
‚úÖ **Lab 5**: Ingress, TLS, External access  
‚úÖ **Lab 6**: RBAC, Security, Network policies  
‚úÖ **Lab 7**: Autoscaling, Resource management  
‚úÖ **Lab 8**: Service mesh, Monitoring, Multi-app  
‚úÖ **Lab 9**: Chaos engineering, Resilience

---

## üöÄ What's Next?

### Practice More
- Run all labs again from scratch
- Modify YAML files and experiment
- Break things and fix them
- Document your learnings

### Contribute
- Share your lab results
- Report issues you found
- Suggest improvements
- Help others learn

### Advanced Topics
- GitOps (ArgoCD, Flux)
- Policy enforcement (OPA, Kyverno)
- eBPF & Cilium
- Multi-cluster management
- Cloud provider integrations

### Certifications
- CKA (Certified Kubernetes Administrator)
- CKAD (Certified Kubernetes App Developer)
- CKS (Certified Kubernetes Security Specialist)

---

## üìö Additional Resources

- **Official Docs**: kubernetes.io/docs
- **Practice**: KodeKloud, A Cloud Guru
- **Community**: Kubernetes Slack, Reddit r/kubernetes
- **Blogs**: kubernetes.io/blog
- **YouTube**: Kubernetes official channel

---

**Thank you for completing this learning journey! Keep practicing and exploring Kubernetes!** üéâ

---

## ü§î Why Does Chaos Engineering Exist?

**The Netflix Chaos Monkey Story (2011)**:  
Netflix moved to AWS. One day, an entire AWS region failed. Result? 3-hour outage for 40 million users. Why? They never tested "what if AWS fails?" in production. After the incident, they built Chaos Monkey to randomly kill servers during business hours.

**Bad Solution ‚ùå**: "Test failover in staging, pray it works in production"  
**Netflix Solution ‚úÖ**: Break production constantly (controlled chaos). If you can survive random pod kills daily, you can survive real outages. Continuous resilience testing, not annual disaster recovery drills.

**Analogy**: No chaos = fire drill once a year, hope you remember during real fire. Chaos engineering = smoke alarm tests every month, muscle memory kicks in during emergency.

**Real-world stat**: Companies practicing chaos engineering have 50% fewer outages and 70% faster recovery times. Netflix hasn't had a regional outage since implementing Chaos Monkey (2011-2024 = 13 years!). (Source: Netflix Tech Blog, Gremlin State of Chaos 2024)

**This lab teaches**: Controlled failure injection, resilience validation, and recovery measurement‚Äîthe three skills that differentiate "we hope it works" from "we KNOW it works."

---

## üé§ Interview Prep: Chaos Engineering

### Q: "Describe a time you tested system resilience in production."

‚ùå **Weak Answer**:  
"I ran chaos experiments to test our system's ability to handle failures."

‚úÖ **STAR Method Answer**:

**Situation**: E-commerce platform processing $2M/day. Management wanted 99.95% uptime SLA but we'd never tested failure scenarios in production. Previous outage (database crash) took 45 minutes to recover because nobody knew the runbook.

**Task**: Validate system could self-heal within 5-minute SLA for P1 incidents. Prove to leadership that infrastructure could handle Black Friday traffic spikes with zero downtime.

**Action**:
1. **Baseline metrics**: Established normal state - p95 latency 180ms, error rate 0.02%, recovery time unknown
2. **Gradual chaos introduction**:
   - **Week 1**: Pod kill (mode: one) during off-peak hours (2 AM). Observed recovery time: 8 seconds (Kubernetes self-healing worked!)
   - **Week 2**: Network delay (300ms) during low-traffic window. Result: Circuit breakers triggered at 500ms, traffic rerouted to healthy pods. Success!
   - **Week 3**: Database CPU stress (50% load) during business hours (got approval!). Result: Read replicas handled load, master stayed healthy. Cache hit rate increased 90% ‚Üí 98%.
   - **Week 4**: Combined chaos (pod kill + network delay) simulating zone failure. Recovery time: 12 seconds, no user-facing errors.
3. **Automated runbooks**: Documented every recovery step. Added PagerDuty playbooks with exact commands.
4. **Chaos in CI/CD**: Added pre-production chaos tests. Deployment blocked if recovery time >30 seconds.

**Result**:
- Validated 5-minute SLA (actual recovery: 12 seconds = 25x better!)
- Discovered 2 issues pre-production: Missing readiness probe on new service, undersized connection pool
- Black Friday zero downtime (10x traffic spike)
- MTTR reduced from 45 min ‚Üí 3 min (93% improvement)
- Earned leadership buy-in for $50K/year Chaos Engineering platform investment

**Key Terms to Use**:
- **Blast radius** (scope of failure impact)
- **Steady state hypothesis** (expected system behavior)
- **Game days** (scheduled chaos exercises with whole team)
- **Observability-driven** (chaos without monitoring = reckless)
- **Progressive rollout** (dev ‚Üí staging ‚Üí off-peak prod ‚Üí business hours)

**What Makes You Stand Out**:
- **Risk management** (gradual rollout, approval for business-hours testing)
- **Quantified impact** (12-second recovery vs 5-minute SLA)
- **Preventive wins** (found 2 bugs before production)
- **Business outcome** (Black Friday success, $2M/day revenue protected)
- **Cultural shift** (automated chaos in CI/CD = normalized failure testing)

---

### Q: "How would you convince leadership to let you run chaos experiments in production?"

‚ùå **Weak Answer**:  
"Show them that Netflix does it and it prevents outages."

‚úÖ **Strong Answer**:

**Start with data, not requests:**

**Phase 1: Build evidence in staging (2 weeks)**
- Run 10 chaos experiments in staging
- Document findings: "Found 3 critical issues that would've caused production outages"
- Calculate cost of prevented outages: 3 outages √ó 30 min each √ó $10K/min lost revenue = **$900K saved**

**Phase 2: Propose limited production test**
- **When**: 2 AM Tuesday (lowest traffic: 50 req/sec vs peak 5,000 req/sec)
- **What**: Single pod kill (mode: one, duration: 10 seconds)
- **Observability**: Full monitoring dashboard, 3 engineers on call
- **Rollback**: Instant experiment deletion if error rate >0.1%
- **Business case**: "If this 10-second test reveals an issue, we prevent a potential 2-hour outage costing $1.2M"

**Phase 3: Share results**
- **Outcome**: "Pod killed at 2:03 AM, recovered in 8 seconds, zero user impact"
- **Learning**: "Discovered connection pooling issue that would've caused 30-second delay during real outage"
- **Request**: "Next experiment: 2 AM Thursday, network delay test (5 minutes)"

**Confidence-building progression:**
1. Staging chaos ‚Üí Find 3 bugs
2. Off-peak single-pod chaos ‚Üí Validate self-healing
3. Off-peak multi-pod chaos ‚Üí Test blast radius
4. Low-traffic business hours ‚Üí Observe under load
5. Game day (scheduled, entire team ready) ‚Üí Full-scale test

**Key insights to share with leadership:**
- "The question isn't IF we'll have an outage, it's WHEN. Chaos engineering moves that WHEN from Black Friday to Tuesday at 2 AM."
- "Every chaos experiment that finds a bug is an outage we prevented. Cost: 5 engineer-hours. Benefit: $10K-$1M saved."
- "Companies that practice chaos engineering recover 70% faster during real incidents. Faster recovery = less revenue loss."

**Red flags to avoid:**
- ‚ùå "Let's just try it and see what happens" (sounds reckless)
- ‚ùå "Everyone's doing chaos engineering" (appeal to authority without context)
- ‚úÖ "Here's our risk mitigation plan and rollback procedure" (shows professionalism)

This approach shows you understand **stakeholder management**, **risk communication**, and **progressive trust-building**‚Äîskills beyond technical expertise!

---

## üß† Quick Check

1. What is a safe first step when introducing chaos experiments into a staging environment?
2. Name two types of NetworkChaos actions supported by Chaos Mesh.
3. Why should you keep chaos experiment durations short when starting out?

## üèÜ Challenge Mode

- Create a short Workflow that runs a pod-kill (mode: one) followed by a 30s network delay. Verify the application remains available (returns 200) during the experiment and document the recovery time.

---

## üß† Test Your Knowledge

Ready to verify your mastery? Take the **[Lab 9 Self-Assessment Quiz](../docs/learning/SELF-ASSESSMENT.md#-lab-9--chaos-engineering)** and see how you score!

